{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterated Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "The Prisoner's dilemma is an example of a game that shows why two rational individuals might not cooperate, even if it appears in their best interest to do so.  \n",
    "The example cites as follows:  \n",
    "\"Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of speaking to or exchanging messages with the other. The police admit they don't have enough evidence to convict the pair on the principal charge. They plan to sentence both to a year in prison on a lesser charge. Simultaneously, the police offer each prisoner a Faustian bargain.\" ([cit](https://www.worldcat.org/title/prisoners-dilemma/oclc/23383657))\n",
    "\n",
    "Each player A and B has two actions: cooperate (C) with the other prisoner or defect (D), giving them up to the autorities. Both players decide on their action without knowing the action of the other player.\n",
    "\n",
    "In the following matrix, the values $(A_{i,j},B_{i,j})$ refer to the payoff A and B, respectively, would get in the case A chose action $i$ and B chose action $j$, where $i,j \\in \\{ \\text{cooperate}, \\text{defect}\\}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Payoff matrix }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline  &  & \\qquad \\text{Prisoner B} \\\\\n",
    " &  & \\text{Cooperate}  & \\text{Defect} \\\\\n",
    "\\text{Prisoner A} & \\text{Cooperate} & (R,R) & (S,T) \\\\\n",
    " & \\text{Defect} & (T,S) & (P,P) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "This table defines a Prisoner’s Dilemma game if $T > R > P > S$ and $2R > S + T$. This way, the dilemma unfolds: For each player it is individually beneficial to defect, irrespective of the choice of his opponent. The highest mutual payoff, however, is generated when both players cooperate.\n",
    "\n",
    "It is assumed that both prisoners understand the nature of the game, have no loyalty to each other, and will have no opportunity for retribution or reward outside the game. \n",
    "\n",
    "### Iterated version\n",
    "If two players play the game more than once in succession and they remember previous actions of their opponent and change their strategy accordingly, the game is called Iterated Prisoner's Dilemma (IPD).  \n",
    "After each game, each player observes both the previous action of their opponent and their own previous action.\n",
    "\n",
    "## Formalisation\n",
    "The prisoner's dilemma is a 2-player Matrix game, has only four states and two choices of actions. Since the reward (payoff) for a given action depends also on the actions of the other player, it is an adversarial bandits problem.  \n",
    "The four states consist of all possible combinations of actions for (A's previous action, B's previous action): $(C,C), (C,D), (D,C), (D,D)$.\n",
    "\n",
    "At each time $t$, the player simultaneously choose their actions. The rewards depend on both actions, and they can be written as the payoff matrix reported above.  \n",
    "\n",
    "Considering the set of possible actions, if for any pair no individual player can benefit by changing its individual strategy, then that's a Nash equilibrium.\n",
    "There has been [research](https://www.pnas.org/doi/10.1073/pnas.1206569109) showing that only one single previous state is needed, in order to define any prisoner’s dilemma strategy.\n",
    "\n",
    "### Q-Learning \n",
    "One agent learns the value function via Q-learning, a reinforcement learning technique developed in $1989$ that iteratively updates expected cumulative discounted reward $Q$ given a state $s$, and a future action $a$ (from [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html)):  \n",
    "<img src=\"Qlearning.png\" width=\"500\">\n",
    "\n",
    "The action gets chosen using a decaying $\\varepsilon$-greedy policy, that is, \n",
    "$$A_t = \\begin{cases} \\max_a Q(A_t) \\quad \\text{with probability } 1- \\varepsilon \\\\ \\text{random action} \\quad \\text{with probability }\\varepsilon \\end{cases}$$\n",
    "\n",
    "As the update rule does not depend on the current exploration but on the assumed optimal choice, Q-Learning does not require the current policy to converge towards the optimal policy, therefore doesn't strictly need the decaying $\\varepsilon$, as for example SARSA would. Q-learning is able to generate an optimal policy even using only uniformly random actions, given sufficient iterations. Nevertheless, \n",
    "\n",
    "The parameters used will be:\n",
    "* The values $R = 3$, $S = 0$, $T =5$, $P=1$ for the payoff matrix\n",
    "* a discount factor $\\gamma = 0.95$\n",
    "* a learning rate $\\alpha = 0.1$\n",
    "* a starting epsilon $\\varepsilon = 0.3$, with a decay of $0.999$ per step, reaching a minimum of $\\varepsilon = 0.1$\n",
    "\n",
    "The payoff matrix is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Payoff matrix }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline  &  & \\qquad \\text{Prisoner B} \\\\\n",
    " &  & \\text{Cooperate}  & \\text{Defect} \\\\\n",
    "\\text{Prisoner A} & \\text{Cooperate} & (3,3) & (0,5) \\\\\n",
    " & \\text{Defect} & (5,0) & (1,1) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Opponent's strategies\n",
    "Over time various strategies in the IPD game emerged. The ones that will be considered here are:\n",
    "* *Always cooperate*: always cooperate, indepentently of period or observed actions. If played against itself, it always recieves an average reward of 3 in this setting.\n",
    "* *Always defect*:  This strategy is by definition unexploitable and will always have at least the same average reward as the opposing strategy.\n",
    "* *Random action*: As the name implies, this strategy plays a random action independent of period and observations. The probability to play a cooperation is here st to $0.5$.\n",
    "* *Tit-for-tat*: This strategy starts with a cooperation and copies the move of the opponent in the following periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from strategy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define payoff matrix\n",
    "payoff =[[(3,3),(0,5)],[(5,0),(1,1)]]\n",
    "\n",
    "# initiate game with two possible actions: cooperate or defect\n",
    "# numeric action codes: [0,1]\n",
    "actions = [0,1] \n",
    "g = Game(payoff, actions)\n",
    "\n",
    "# returns a list of Nash equilibrium indexes\n",
    "g.getNash() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nash equilibrium is for both to defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooperate 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "defect 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "\n",
      "random 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test action classes\n",
    "for strategy in [Cooperate(), Defect(), Random()]: \n",
    "    s1 = strategy\n",
    "    print(s1.name,end=\" \")\n",
    "    for i in range (0,20):\n",
    "        print(s1.get_action(i), end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitforTat\t0 1 0 1 1 1 0 1 1 1 = 15\n",
      "random\t1 0 1 1 1 0 1 1 1 1 = 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TitforTat needs to be tested against a opponent, in this case, random\n",
    "m = Meeting(g, TitforTat(), Random(), 10)\n",
    "m.run()\n",
    "m.pretty_print()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitforTat\t0 1 1 1 1 1 1 1 1 1 = 9\n",
      "defect\t1 1 1 1 1 1 1 1 1 1 = 14\n",
      "\n",
      "Number of cooperations : \n",
      "TitforTat\t1\n",
      "defect\t0\n"
     ]
    }
   ],
   "source": [
    "# let's do a meeting\n",
    "# The score of each is the sum of the scores obtained on each move, according to the game matrix.\n",
    "\n",
    "s1 = TitforTat()\n",
    "s2 = Defect()\n",
    "m = Meeting(g, s1, s2, 10)\n",
    "m.run()\n",
    "m.pretty_print()\n",
    "\n",
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table:  [[[50.05843594 60.81721848]\n",
      "  [57.84633015 54.27169493]]\n",
      "\n",
      " [[57.0631519  61.63072538]\n",
      "  [58.39043986 60.80935461]]]\n",
      "QLearning\t0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 ... = 28454\n",
      "random\t1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 0 1 ... = 8964\n"
     ]
    }
   ],
   "source": [
    "s1 = QLearning()\n",
    "s2 = Random()\n",
    "m = Meeting(g, s1, s2, 10000)\n",
    "m.run()\n",
    "m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of cooperations : \n",
      "QLearning\t1140\n",
      "random\t5038\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e94127da5e596dc8d6210f9f94bfee13570c6bfd49eef8a896a40dfe8eb2aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
