{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterated Prisoner's Dilemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "The Prisoner's dilemma is an example of a game that shows why two rational individuals might not cooperate, even if it appears in their best interest to do so.  \n",
    "The example cites as follows:  \n",
    "\"Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of speaking to or exchanging messages with the other. The police admit they don't have enough evidence to convict the pair on the principal charge. They plan to sentence both to a year in prison on a lesser charge. Simultaneously, the police offer each prisoner a Faustian bargain.\" ([cit](https://www.worldcat.org/title/prisoners-dilemma/oclc/23383657))\n",
    "\n",
    "Each player A and B has two actions: cooperate (C) with the other prisoner or defect (D), giving them up to the autorities. Both players decide on their action without knowing the action of the other player.\n",
    "\n",
    "In the following matrix, the values $(A_{i,j},B_{i,j})$ refer to the payoff A and B, respectively, would get in the case A chose action $i$ and B chose action $j$, where $i,j \\in \\{ \\text{cooperate}, \\text{defect}\\}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Payoff matrix }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline  &  & \\qquad \\text{Prisoner B} \\\\\n",
    " &  & \\text{Cooperate}  & \\text{Defect} \\\\\n",
    "\\text{Prisoner A} & \\text{Cooperate} & (R,R) & (S,T) \\\\\n",
    " & \\text{Defect} & (T,S) & (P,P) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "This table defines a Prisoner’s Dilemma game if $T > R > P > S$ and $2R > S + T$. This way, the dilemma unfolds: For each player it is individually beneficial to defect, irrespective of the choice of his opponent. The highest mutual payoff, however, is generated when both players cooperate.\n",
    "\n",
    "It is assumed that both prisoners understand the nature of the game, have no loyalty to each other, and will have no opportunity for retribution or reward outside the game. \n",
    "\n",
    "### Iterated version\n",
    "If two players play the game more than once in succession and they remember previous actions of their opponent and change their strategy accordingly, the game is called Iterated Prisoner's Dilemma (IPD).  \n",
    "After each game, each player observes both the previous action of their opponent and their own previous action.\n",
    "\n",
    "## Formalisation\n",
    "The prisoner's dilemma is a 2-player Matrix game, has only four states and two choices of actions. Since the reward (payoff) for a given action depends also on the actions of the other player, it is an adversarial bandits problem.  \n",
    "The four states consist of all possible combinations of actions for (A's previous action, B's previous action): $(C,C), (C,D), (D,C), (D,D)$.\n",
    "\n",
    "At each time $t$, the player simultaneously choose their actions. The rewards depend on both actions, and they can be written as the payoff matrix reported above.  \n",
    "\n",
    "Considering the set of possible actions, if for any pair no individual player can benefit by changing its individual strategy, then that's a Nash equilibrium.\n",
    "There has been [research](https://www.pnas.org/doi/10.1073/pnas.1206569109) showing that only one single previous state is needed, in order to define any prisoner’s dilemma strategy.\n",
    "\n",
    "### Q-Learning \n",
    "One agent learns the value function via Q-learning, a reinforcement learning technique developed in $1989$ that iteratively updates expected cumulative discounted reward $Q$ given a state $s$, and a future action $a$ (from [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html)):  \n",
    "<img src=\"Qlearning.png\" width=\"500\">\n",
    "\n",
    "The action gets chosen using a decaying $\\varepsilon$-greedy policy, that is, \n",
    "$$A_t = \\begin{cases} \\max_a Q(A_t) \\quad \\text{with probability } 1- \\varepsilon \\\\ \\text{random action} \\quad \\text{with probability }\\varepsilon \\end{cases}$$\n",
    "\n",
    "As the update rule does not depend on the current exploration but on the assumed optimal choice, Q-Learning does not require the current policy to converge towards the optimal policy, therefore doesn't strictly need the decaying $\\varepsilon$, as for example SARSA would. Q-learning is able to generate an optimal policy even using only uniformly random actions, given sufficient iterations. Nevertheless, \n",
    "\n",
    "The parameters used will be:\n",
    "* The values $R = 3$, $S = 0$, $T =5$, $P=1$ for the payoff matrix\n",
    "* a discount factor $\\gamma = 0.95$\n",
    "* a learning rate $\\alpha = 0.1$\n",
    "* a starting epsilon $\\varepsilon = 0.3$, with a decay of $0.999$ per step, reaching a minimum of $\\varepsilon = 0.1$\n",
    "\n",
    "The payoff matrix is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {Payoff matrix }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline  &  & \\qquad \\text{Prisoner B} \\\\\n",
    " &  & \\text{Cooperate}  & \\text{Defect} \\\\\n",
    "\\text{Prisoner A} & \\text{Cooperate} & (3,3) & (0,5) \\\\\n",
    " & \\text{Defect} & (5,0) & (1,1) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### SARSA\n",
    "The agent can also learn the value function via SARSA. The Sarsa algorithm is an on-policy algorithm that uses TD learning method to estimate the Q table. The Sarsa update method is given by:\n",
    "\n",
    "<img src=\"Sarsa.png\" width=\"500\">\n",
    "\n",
    "Differently from Q-learning, Sarsa considers which action is actually taken when updating the action-value function Q.\n",
    "\n",
    "### Opponent's strategies\n",
    "Over time various strategies in the IPD game emerged. The ones that will be considered here are:\n",
    "* *Always cooperate*: always cooperate, indepentently of period or observed actions. If played against itself, it always recieves an average reward of 3 in this setting.\n",
    "* *Always defect*:  This strategy is by definition unexploitable and will always have at least the same average reward as the opposing strategy.\n",
    "* *Random action*: As the name implies, this strategy plays a random action independent of period and observations. The probability to play a cooperation is here st to $0.5$.\n",
    "* *Tit-for-tat*: This strategy starts with a cooperation and copies the move of the opponent in the following periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all needed classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from utils import Game, Meeting\n",
    "from strategy import Cooperate, Defect, Random, TitforTat, QLearning, Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the payoff matrix, and the possible actions. We can instantiate a first game with them, and get the Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define payoff matrix\n",
    "payoff =[[(3,3),(0,5)],[(5,0),(1,1)]]\n",
    "\n",
    "# initiate game with two possible actions: cooperate or defect\n",
    "# numeric action codes: [0,1]\n",
    "actions = [0,1] \n",
    "g = Game(payoff, actions)\n",
    "\n",
    "# returns a list of Nash equilibrium indexes\n",
    "g.getNash() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Nash equilibrium is to defect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the strategies play out. Remember that 0 = Cooperate, 1 = Defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooperate 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "\n",
      "defect 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "\n",
      "random 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test action classes\n",
    "for strategy in [Cooperate(), Defect(), Random()]: \n",
    "    s1 = strategy\n",
    "    print(s1.name,end=\" \")\n",
    "    for i in range (0,20):\n",
    "        print(s1.get_action(i), end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test Tit-for-Tat, we need an opponent, so we create a meeting. The player starts by cooperating, to then copy the opponent's previous action.  \n",
    "```pretty_print()``` reports the first (max 50) actions for each player and their scores. The score is the sum of the scores obtained on each game, according to the payoff matrix. The higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitforTat\t0 1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 = 35\n",
      "random\t1 1 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 = 40\n"
     ]
    }
   ],
   "source": [
    "m = Meeting(g, TitforTat(), Random(), 20)\n",
    "m.run()\n",
    "m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the counter for how many times each player decided to cooperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TitforTat\t0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ... = 99\n",
      "defect\t1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ... = 104\n",
      "\n",
      "Number of cooperations : \n",
      "TitforTat\t1\n",
      "defect\t0\n"
     ]
    }
   ],
   "source": [
    "s1 = TitforTat()\n",
    "s2 = Defect()\n",
    "m = Meeting(g, s1, s2, 100)\n",
    "m.run()\n",
    "m.pretty_print()\n",
    "\n",
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "We can play our Q-learning agent against any other strategy. We can also visualise the final Q-table.  \n",
    "Keep in mind that for ```Random()```, ```QLearning()``` and ```Sarsa()``` randomness plays an important role, therefore the results vary at every run. Setting a seed would mean possibly not observing the most common case, so I preferred to keep it random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLearning\t0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... = 39880\n",
      "cooperate\t0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... = 15180\n",
      "\n",
      "Number of cooperations : \n",
      "QLearning\t5060\n",
      "cooperate\t10000\n"
     ]
    }
   ],
   "source": [
    "s1 = QLearning()\n",
    "s2 = Cooperate()\n",
    "m = Meeting(g, s1, s2, 10000)\n",
    "m.run()\n",
    "m.pretty_print(max = 50)\n",
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[47.42859877,  0.        ],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.        , 78.94624755],\n",
       "        [ 0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to read it:\n",
    "# first as states:\n",
    "# [[I cooperate],         in each block:  \n",
    "#                         [they cooperate],                   in each row, for action:\n",
    "#  [I defect   ]]         [they defect   ]                    [next is cooperate, next is defect]\n",
    "\n",
    "m.s1.print_qtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play every strategy against every opponent, counting how many times our agent (Player 1) wins out of 1000 rounds played.   \n",
    "Again, each time the cell is ran the results can vary due to randomness, although the decision to run the same meeting 1000 times to infer the results was made to reduce the effect.\n",
    "\n",
    "Note: this next cell takes a few minutes to run (~)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooperate\n",
      "defect\n",
      "random\n",
      "TitforTat\n"
     ]
    }
   ],
   "source": [
    "payoff =[[(3,3),(0,5)],[(5,0),(1,1)]]\n",
    "actions = [0,1] \n",
    "g = Game(payoff, actions)\n",
    "\n",
    "strategies = [Cooperate(), Defect(), Random(), TitforTat(), QLearning(), Sarsa()]\n",
    "results_table = []\n",
    "runs = 1000\n",
    "\n",
    "for player1 in strategies:\n",
    "    results = []\n",
    "    for player2 in strategies:\n",
    "        wins = 0\n",
    "        for i in range(runs):\n",
    "            s1 = player1\n",
    "            s2 = player2\n",
    "            m = Meeting(g, s1, s2, 10000)\n",
    "            m.run()\n",
    "            if m.s1_score > m.s2_score:\n",
    "                wins +=1\n",
    "        results.append(wins/runs)\n",
    "    print(player1.name)\n",
    "    results_table.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results_table, index = [s.name for s in strategies], columns = [s.name for s in strategies])\n",
    "s = sn.heatmap(df, annot=True, cmap = 'Blues')\n",
    "s.set(xlabel='PLAYER 2', ylabel='PLAYER 1', title='\\% of Player 1 wins against Player 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the table, always cooperating, not knowing the opponent's strategy, is never a good idea. As the Nash equilibrium suggests, the best bet is to always defect. That way, whichever action Player 2 chooses, Player 1's payoff will always be the highest possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see in particular some interesting meetings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning vs Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = QLearning()\n",
    "s2 = QLearning()\n",
    "m = Meeting(g, s1, s2, 10000)\n",
    "m.run()\n",
    "m.pretty_print()\n",
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning vs Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = QLearning()\n",
    "s2 = Sarsa()\n",
    "m = Meeting(g, s1, s2, 10000)\n",
    "m.run()\n",
    "m.pretty_print()\n",
    "print()\n",
    "print(\"Number of cooperations : \" )\n",
    "print (m.s1.name + \"\\t\" + str(m.num_cooperation_s1))\n",
    "print (m.s2.name + \"\\t\" + str(m.num_cooperation_s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both algorithms tend to balance with $50\\%$ cooperations, as the number of iterations increase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot_cooperation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e94127da5e596dc8d6210f9f94bfee13570c6bfd49eef8a896a40dfe8eb2aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
